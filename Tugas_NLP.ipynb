{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhibuddin12/tensorFlow/blob/master/Tugas_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enkn_Qcrfxqj",
        "outputId": "1df6cebd-9863-443a-9ebf-a276e76d7d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: indo-minang.pkl\n",
            "[keistimewaan tersebut dapat diketahui dari keaslian karangan dan bahasa hamka] => [kaistimewaan tu dapek dikatahui dari kaaslian karangan jo bahaso hamka]\n",
            "[cherrybelle beat indonesia adalah salah satu konser tur keliling pertama grup musik asal indonesia cherrybelle] => [cherrybelle beat indonesia adolah salah satu konser tur kaliliang patamo grup musik asa indonesia cherrybelle]\n",
            "[makamnya diziarahi oleh ribuan pengagumnya dari masa ke masa] => [makamnyo diziarahi dek ribuan pangagumnyo dari maso ka maso]\n",
            "[talmaciu adalah sebuah kota yang terletak di county sibiu rumania tengah] => [talmaciu adolah sabuah kota nan ado di county sibiu rumania]\n",
            "[akhir dari bulan ramadan dirayakan dengan sukacita oleh seluruh muslim di seluruh dunia] => [akhia dari bulan ramadan dirayoan jo sanang ati dek saluruah muslim di saluruah dunia]\n",
            "[adat perpatih merupakan salah satu bentuk sistem demokrasi melayu] => [adaik parpatiah marupoan salah satu bantuak sistem demokrasi malayu]\n",
            "[studi tersebut membahas keberadaan sesuatu yang bersifat konkret] => [studi tasabuik mambahas kabaradaan sesuatu nan basifaik konkret]\n",
            "[air ini digunakan untuk mencuci tangan sebelum dan setelah makan] => [aia ko biaso dipakai untuak mambasuah tangan sabalun jo sasudah makan]\n",
            "[labalaba ini biasanya banyak ditemui di kaledonia baru] => [lawah iko biasonyo banyak ditamui di new caledonia]\n",
            "[muara langsat merupakan salah satu desa di kecamatan benai kabupaten kuantan singingi provinsi riau indonesia] => [muara langsat adolah marupoan desa di kabupaten kuantan singingi provinsi riau indonesia]\n",
            "[tetapi kini terdapat perbedaan dalam hal budaya dan bahasa di antara para penduduknya] => [tapi kini ado pabedaan dalam hal budayo jo bahaso di antaro panduduaknyo]\n",
            "[pintu rime gayo adalah sebuah kecamatan di kabupaten bener meriah aceh indonesia] => [pintu rime gayo adolah marupoan kecamatan di kabupaten bener meriah provinsi nanggroe aceh darussalam indonesia]\n",
            "[dari hasil pernikahan keduanya desy dikarunia seorang anak nasywa nathania hamzah lahir juli] => [dari hasil pernikahan keduanya desy dikarunia saurang anak nasywa nathania hamzah lahir juli]\n",
            "[labalaba ini biasanya banyak ditemui di mongolia] => [lawah iko biasonyo banyak ditamui di mongolia]\n",
            "[dapat digunakan untuk membedakan ion logam yang lain yang akan diendapkan dengan ion karbonat] => [dapek digunokan untuak mambedakan ion logam yang lain nan akan diendapkan jo ion karbonat]\n",
            "[institut teknologi telkom sebelumnya adalah sekolah tinggi teknologi telkom stt telkom] => [institut teknologi telkom sabalumnyo adolah sikolah tinggi teknologi telkom stt telkom]\n",
            "[nama ilmiah dari spesies ini pertama kali diterbitkan pada tahun oleh simon] => [namo ilmiah dari spesies ko partamo kali ditabikan pado taun dek simon]\n",
            "[politeknik negeri semarang saat ini memiliki lima jurusan yang membuka jenjang yaitu sectionteknik sipil] => [politeknik nagari semarang saat kini mamiliki limo juruihan nan mambukak janjang yoitu]\n",
            "[anggota aboean goeroegoeroe dipungut iuran setiap bulan sebagai simpanan yang diperuntukkan untuk mentjahari keonetoengann jang halal] => [anggota aboean goeroegoeroe dimintak pitih tiok bulan nan diparuntukan untuak mentjahari keonetoengann jang halal]\n",
            "[adalah wakil bupati kabupaten padang pariaman sumatra barat periode] => [adolah wakia bupati kabupaten padang pariaman sumatra barat periode]\n",
            "[tangkerang utara adalah salah satu kelurahan di kecamatan bukit raya pekanbaru riau indonesia] => [tangkerang utara adolah salah satu kelurahan di kecamatan bukit raya pekanbaru riau indonesia]\n",
            "[plastik dapat dibentuk menjadi film atau fiber sintetik] => [asoy dapek dibantuak manjadi film atau fiber sintetik]\n",
            "[memasuki tahun pemerintah mengumumkan kenaikkan tdl tarif telepon dan bbm secara serentak] => [mamasuaki taun pamarentah maumuman kanaiakan tdl tarif talepon jo bbm sacaro sarantak]\n",
            "[bptu padang mengatas pertama kali didirikan oleh pemerintah hindia belanda pada tahun] => [bptu padang mangateh patamo kali didirian dek pamarintah hindia belanda pado taun]\n",
            "[prakarsa abadi press medan dan ketua yayasan asma cabang sumatra utara] => [prakarsa abadi press medan dan ketua yayasan asma cabang sumatera utara]\n",
            "[paleoseanografi mempelajari sejarah lautan dalam artian sejarah geologinya] => [mampelajari sijarah lautan dalam artian sijarah geologinyo]\n",
            "[kabupaten solok bukanlah daerah baru karena solok telah ada jauh sebelum undangundang pembentukan wilayah ini dikeluarkan] => [kabupaten solok bukanlah daerah baru karano solok alah ado jauah sabalum undangundang pambantuakan wilayah dikaluakan]\n",
            "[kota tinggi adalah salah satu kelurahan di kecamatan pekanbaru kota pekanbaru riau indonesia] => [kota tinggi adolah salah satu kelurahan di kecamatan pekanbaru kota pekanbaru riau indonesia]\n",
            "[johnny anwar adalah kakak dari rosihan anwar seorang tokoh wartawan dan pendiri koran pedoman] => [johnny anwar adolah kakak dari rosihan anwar surang tokoh wartawan sarato pandiri koran pedoman]\n",
            "[sekolah ini dikhususkan bagi yang telah menamatkan pendidikannya di sumatra thawalib diniyah dan tarbiyah] => [sakolah iko dikhususan untuak nan alah manamaikan pandidikannyo di sumatra thawalib diniyah jo tarbiyah]\n",
            "[sebagai perbandingan antarktika hampir dua kali ukuran australia] => [sabagai pabandiangan antarktika ampia duo kali ukuran australia]\n",
            "[oktober adalah bulan kesepuluh tahun dalam kalender gregorian] => [oktober adolah bulan kasapuluah dalam taun kalender gregorian]\n",
            "[ryana dea maharani yang lebih dikenal sebagai ryana dea adalah aktris dan model berkebangsaan indonesia] => [ryana dea maharani nan labiah dikenal sabagai ryana dea adolah aktris indonesia]\n",
            "[di jakarta pedagang nasi kapau berjejer di jalan kramat raya jakarta pusat] => [di jakarta panjua nasi kapau rami manggaleh di jalan kramat raya jakarta pusat]\n",
            "[theory and history of ontology] => [theory and history of ontology]\n",
            "[bukit napa kuranji kec] => [bukit napa kuranji kec]\n",
            "[setelah menjalani putaran kedua pada maret mahyeldi kembali unggul dengan perolehan suara] => [satalah manjalani putaran kaduo pado maret mahyeldi kambali unggua jo parolehan suaro]\n",
            "[jafar ditangkap pada mei di surabaya saat ia dalam perjalanan dari ambon] => [jafar ditangkok pado mei di surabaya saaik inyo dalam pajalanan dari ambon]\n",
            "[kemudian berkeliaran di hutanhutan sumatra setelah pemerintah pusat menggempur kekuatan prri] => [sasudah tu bakaliaran di hutanhutan sumatera sasudah pamarentah pusat manggempur kakuatan prri]\n",
            "[mengikuti banyak penulis yang menggunakan nama pena maka ia menyingkat nama kecilnya menjadi sebuah nama pena upita agustine] => [dek banyak panulih manggunoan namo pena mako inyo pun mangunoan namo pena upita agustine]\n",
            "[dalam hal pemilihan waktu permainan indang ini terkenal pula dengan istilah indang naik dan indang turun] => [dalam pamiliahan wakatu pamainan indang ko terkenal pulo jo istilah indang naiak jo indang turun]\n",
            "[eksentrisitas orbit asteroid ini tercatat sebesar sementara magnitudo mutlaknya adalah] => [eksentrisitas orbit asteroid ko tacatat sagadang samantaro magnitudo mutlaknyo adolah]\n",
            "[kabupaten mandailing natal berbatasan dengan provinsi sumatra barat] => [kabupaten mandailing natal babatehan joprovinsi sumatera barat]\n",
            "[tottenham hotspur fc] => [tottenham hotspur fc]\n",
            "[sma negeri pariaman memiliki agenda tahunan yaitu ipa cup] => [sma negeri pariaman mamiliki agenda tahunan yoitu ipa cup]\n",
            "[katedral ini dipandang sebagai salah satu contoh terbaik arsitektur gothik prancis] => [katedral ko dipandang sabagai salah satu contoh arsitektur gothik parancih tarancak]\n",
            "[masyarakat di luar sumatra barat lebih sering menyebutnya sebagai martabak kubang] => [masyarakaik di lua sumatera barat labiah acok manyabuiknyo sabagai martabak kubang]\n",
            "[secara administratif orkney merupakan sebuah kabupaten skotlandia yang berstatus kepulauan] => [sacaro administratif orkney marupoan sabuah kabupaten di skotlandia nan statusnyo tu kapulauan]\n",
            "[ria terkenal sebagai penyanyi pertama yang mempopulerkan lagu sms yang pernah meledak di pasaran] => [ria dikana sabagai panyanyi patamo nan mampopulerkan lagu sms nan pernah rami di pasaran]\n",
            "[selama masa pendudukan jepang abikoesno tjokrosoejoso adalah tokoh kunci dalam masyumi] => [salamo maso pandudukan jepang abikoesno tjokrosoejoso adolah tokoh kunci dalam masyumi]\n",
            "[dia berasal dari kedah] => [inyo barasl dari kedah]\n",
            "[di tv one karni menjabat sebagai direktur pemberitaan atau pemimpin redaksi news dan sports] => [di tv one karni manjabat sabagai direktur pemberitaan atau pemimpin redaksi news dan sports]\n",
            "[bumbu gulai masin diantaranya santan serai cabai daun salam dan daun jeruk] => [bumbu gulai masin di antaronyo santan sarai lado daun salam jo daun limau]\n",
            "[pada tahun ia menerima penghargaan bessies award dari new york dance and performance] => [pado taun inyo mandapek panghargaan bessies award dari new york dance and performance]\n",
            "[rosihan merupakan anak keempat dari sepuluh bersaudara pasangan anwar maharaja sutan dan siti safiah] => [rosihan adolah anak kaampek diantaro sapuluah basudaro dari pasangan anwar maharaja sutan jo siti safiah]\n",
            "[di bawah kepemimpinan emir qatar hamad bin khalifa althani qatar mengalami liberalisasi] => [di bawah kapamimpinan emir qatar hamad bin khalifa althani qatar mangalami modernisasi dan liberalisasi]\n",
            "[aparat pemerintah menyita koran oposisi partai islam semalaysia itu dari lapaklapak] => [aparaik pamarentah manyita koran oposisi partai islam semalaysia tu dari lapaklapak]\n",
            "[bangunan kedua adalah mandapa dengan ukuran x m dan berdenah empat persegi panjang] => [bangunan kaduo adolah mandapa dengan ukuran x m dan badenah ampat pasegi panjang]\n",
            "[pada sekolah ini menggunakan kurikulum sebelumnya dengan kurikulum tingkat satuan pendidikan] => [pado sikolah iko manggunokan kurikulum sabalunnyo jo kurikulum tingkat satuan pendidikan]\n",
            "[abdul muluk adalah putra dari abdul hamid syah sultan barbari] => [abdul muluk adolah putra dari abdul hamid syah sultan barbari]\n",
            "[di studio ini mereka menyanyi dan merekamnya dalam cd untuk koleksi pribadi] => [di studio ko mareka manyanyi jo marakamnyo dalam cd untuak koleksi pribadi]\n",
            "[dengan demikian politeknik menghasilkan lulusan yang mampu melakukan tugastugas di industri secara profesional] => [jo damikian politeknik manghasilkan lulusan nan mampu malakukan tugastugas di industri sacara profesional]\n",
            "[gelar ini sekaligus juga menjadikannya sebagai doktor wanita pertama di program pascasarjana ipb] => [gala iko sakaligus manjadiannyo sabagai doktor padusi partamo di program pascasarjana ipb]\n",
            "[ayahnya adalah seorang profesor di universitas tersebut dan ibunya adalah seorang guru] => [apaknyo adolah saurang profesor di universitas tasabuik dan ibunya adolah saurang guru]\n",
            "[nama ilmiah dari spesies ini pertama kali diterbitkan pada tahun oleh chamberlin ivie] => [namo ilmiah dari spesies ko partamo kali ditabikan pado taun dek chamberlin ivie]\n",
            "[wafat di pekanbaru mei] => [baliau maningga di pekanbaru mei]\n",
            "[bukit raya adalah sebuah kecamatan di kota pekanbaru riau indonesia] => [bukit raya adolah sabauh kecamatan di kota pekanbaru riau indonesia]\n",
            "[terdapat sebanyak anak tangga di atas loteng yang digunakan untuk mencapai puncak surau] => [tadapek sabanyak anak tangga di ateh loteang nan digunoan untuak mancapai puncak surau]\n",
            "[glenea apicespinosa adalah spesies kumbang tanduk panjang yang tergolong familia cerambycidae] => [glenea apicespinosa adolah kumbang tanduak panjang dari famili cerambycidae]\n",
            "[junisaf pernah menjadi pemimpin redaksi antara sedangkan yozar terkenal sebagai pimpinan eksponen] => [junisaf pernah manjadi pamimpin redaksi antara sadangkan yozar dikana sabagai pimpinan eksponen]\n",
            "[program adiwiyata telah menciptakan warga sekolah yang peduli dan berbudaya lingkungan] => [program adiwiyata alah menciptakan warga sikolah nan paduli dan babudayo lingkuangan]\n",
            "[tulus juga berprofesi sebagai seorang arsitek setelah menamatkan studinya di universitas katolik parahyangan bandung] => [tulus juo baprofesi sabagai saurang arsitek salapeh manamaikan studinyo di universitas katolik parahyangan bandung]\n",
            "[selain itu dotona davidi juga dapat menyerap oksigen dari air melalui proses difusi] => [salain itu dotona davidi dapek juo manyerap oksigen dari aia malalui proses difusi]\n",
            "[fahmi yang menghabiskan masa kecilnya di kenari jakarta pusat terkenal bengal dan suka berkelahi] => [fahmi nan maabihan maso keteklnyo di kenari jakarta pusat dikana tangka sarato suko bacakak]\n",
            "[pagindar adalah sebuah kecamatan di kabupaten pakpak bharat sumatra utara indonesia] => [pagindar adolah marupoan sabuah kecamatan nan ado di kabupaten pakpak bharat provinsi sumatera utara indonesia]\n",
            "[tradisi baroncah ini telah lama ada dalam kehidupan masyarakatpadang sibusuk sijunjung sumatra barat] => [tradisi baroncah alah lamao ado di nagari padang sibusuak sijunjung sumatra barat]\n",
            "[pada zaman revolusi kemerdekaan kegiatannya terhenti] => [pado zaman revolusi kemerdekaan kagiatannyo terhenti]\n",
            "[walaupun namanya pacu jawi balapan sapi dalam bahasa minang acara ini sebenarnya bukan lomba adu kecepatan sapi] => [walaupun namonyo pacu jawi acara iko sabananyo indak lomba adu kancang lari jawi]\n",
            "[ia merupakan orang indonesia pertama yang mendapatkan ijazah setingkat itu] => [baliau adolah urang indonesia partamo nan mandapekan ijazah satingkek itu]\n",
            "[pemain dengan tinggi badan dan berat itu menempati posisi gelandang dalam timnya] => [pamain nan batinggi badan jo barek tu manampeki posisi galandang dalam timnyo]\n",
            "[eksentrisitas orbit asteroid ini tercatat sebesar sementara magnitudo mutlaknya adalah] => [eksentrisitas orbit asteroid ko tacatat sagadang samantaro magnitudo mutlaknyo adolah]\n",
            "[raja saud gemar membelanjakan uang negara demi kepentingan pribadi dan keluarganya] => [rajo saud gemar mambalanjoan pitih nagara demi kapentingan pribadi jo keluarganyo]\n",
            "[tiga di mudik empat dengan tanah rawang] => [tiga di mudik empat dengan tanah rawang]\n",
            "[kariernya cepat melesat pada tahun dia dipercaya menjadi demang bukittinggi] => [kariernyo capek naiak di taun baliau dipacayo manjadi damang bukittinggi]\n",
            "[biarpun begitu namanya masih dikenal para penikmat novel wuxia] => [biapun baitu namonyo masih dikana dek para pacintonovel wuxia]\n",
            "[pc menjadi populer untuk banyak tugas termasuk menyimpan buku menulis dan mencetak dokumen] => [pc manjadi populer untuak banyak tugas tamasuak manyimpan buku manuliah jo mancetak dokumen]\n",
            "[klub ini memainkan pertandingan kandangnya di stadion ennio tardini yang berkapasitas penonton] => [klub iko mamainan patandiangan kandangnyo di ennio tardini nan bakapasitas panonton]\n",
            "[britania raya mundur dari bahrain pada bulan agustus menjadikan bahrain sebagai sebuah negara merdeka] => [britania rayo mundua dari bahrain pado bulan agustus manjadikan bahrain sabagai negari nan mardeka]\n",
            "[spesies ini juga merupakan bagian dari ordo araneae] => [spesies ko juo marupokan bagian dari genus gonatium dan ordo araneae]\n",
            "[aghlabiyyah adalah sebuah dinasti arab penguasa ifriqiyyah saat itu mencakup tunisia aljazair timur dan tripolitania dan sisilia yang berkuasa antara masehi] => [aghlabiyyah adolah ciek dinasti arabberber panguaso ifriqiyyah aljazair jo sisilia nan bakuaso antaro masehi]\n",
            "[brachyrhopala melaena adalah spesies lalat yang tergolong famili asilidae] => [brachyrhopala melaena adolah saikua langau dari famili asilidae]\n",
            "[ia juga seorang pimpinan adalet ve kalknma partisi akp atau partai keadilan dan pembangunan] => [inyo juo saurang pimpinan adalet ve kalknma partisi akp atau partai kaadilan jo pambangunan]\n",
            "[tahuntahun setelah kemerdekaan ditandai dengan kelaparan bencana alam kemiskinan huruhara politik korupsi dan kudeta militer] => [tahuntahun salapeh kamardekaan ditandoi jo kalaparan bancano alam kamiskinan huruhara politik korupsi jo kudeta militer]\n",
            "[di samping itu ekornya juga mirip buah kemukus yang dikeringkan] => [di sampiang itu ikuanyo juga saroman buah kemukus yang dikariangkan]\n",
            "[ia pernah bergabung dengan himpunan mahasiswa islam hmi jakarta] => [liau panah bagabuang jo himpunan mahasiswa islam hmi jakarta]\n",
            "[soedjojono dan affandi pada masa pendudukan jepang] => [soedjojono jo affandi pado maso panduduakan japang]\n",
            "[bangunannya ditopang oleh tonggak yang terbuat dari kayu laban salah satu kayu pengganti jati] => [bangunannyo ditopang dek tonggak nan tabuek dari kayu laban kayu pangganti jati]\n",
            "[upiak isil mengawali kariernya sejak usia tahun sebagai penyanyi daerah di sumatra barat] => [upiak isil mangawali kariernyo sajak umua taun sabagai penyanyi daerah di sumatera barat]\n",
            "[posisi alami nya adalah striker] => [posisi alami nyo adolah panyarang]\n",
            "[fraksi reformasi menyatakan desakan pembatalan tiga agenda ini] => [fraksi reformasi manyatoan dasakan pambatalan tigo agenda ko]\n",
            "Saved: indo-minang-both.pkl\n",
            "Saved: indo-minang-train.pkl\n",
            "Saved: indo-minang-test.pkl\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'corpus_15_baru.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into indonesia-minang pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'indo-minang.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "  print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('indo-minang.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[1:9000], dataset[9000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'indo-minang-both.pkl')\n",
        "save_clean_data(train, 'indo-minang-train.pkl')\n",
        "save_clean_data(test, 'indo-minang-test.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9Awb0jHNhNlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2699544d-0e49-4860-d89c-f3ed94d7e2c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indo Vocabulary Size: 13456\n",
            "Indo Max Length: 21\n",
            "Minang Vocabulary Size: 15624\n",
            "Minang Max Length: 15\n"
          ]
        }
      ],
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('indo-minang-both.pkl')\n",
        "train = load_clean_sentences('indo-minang-train.pkl')\n",
        "test = load_clean_sentences('indo-minang-test.pkl')\n",
        "\n",
        "# prepare indonesia tokenizer\n",
        "ind_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "ind_vocab_size = len(ind_tokenizer.word_index) + 1\n",
        "ind_length1 = max_length(dataset[:, 0])\n",
        "ind_length = 9\n",
        "print('Indo Vocabulary Size: %d' % ind_vocab_size)\n",
        "print('Indo Max Length: %d' % (ind_length1))\n",
        "# prepare minang tokenizer\n",
        "min_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "min_vocab_size = len(min_tokenizer.word_index) + 1\n",
        "min_length1 = max_length(dataset[:, 1])\n",
        "min_length = 9\n",
        "print('Minang Vocabulary Size: %d' % min_vocab_size)\n",
        "print('Minang Max Length: %d' % (min_length1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aYtHxA-hFItr"
      },
      "outputs": [],
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(min_tokenizer, min_length, train[:, 1])\n",
        "trainY = encode_sequences(ind_tokenizer, ind_length, train[:, 0])\n",
        "trainY = encode_output(trainY, ind_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KmiAraM0FtiV"
      },
      "outputs": [],
      "source": [
        "# prepare validation data\n",
        "testX = encode_sequences(min_tokenizer, min_length, test[:, 1])\n",
        "testY = encode_sequences(ind_tokenizer, ind_length, test[:, 0])\n",
        "testY = encode_output(testY, ind_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nRA3axxmFl9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efbfca89-4e3c-4415-9bd5-1b7840f4b82f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 9, 256)            3999744   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVect  (None, 9, 256)           0         \n",
            " or)                                                             \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 9, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 9, 13456)         3458192   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,508,560\n",
            "Trainable params: 8,508,560\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "134/134 - 105s - loss: 7.7327 - 105s/epoch - 782ms/step\n",
            "Epoch 2/5\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "134/134 - 98s - loss: 7.0869 - 98s/epoch - 730ms/step\n",
            "Epoch 3/5\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "134/134 - 98s - loss: 6.9412 - 98s/epoch - 734ms/step\n",
            "Epoch 4/5\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "134/134 - 98s - loss: 6.8130 - 98s/epoch - 733ms/step\n",
            "Epoch 5/5\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "134/134 - 105s - loss: 6.6979 - 105s/epoch - 786ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1949f49e90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# define model\n",
        "model = define_model(min_vocab_size, ind_vocab_size, min_length, ind_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=5, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cBzyigRfhoqx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "bae5628d-cae7-4d0f-9d28-6b060da884ed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1aa519f8ed35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;31m# test on some training sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at model.h5"
          ]
        }
      ],
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, ind_tokenizer, source)\n",
        "\t\traw_target, raw_src = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('indo-minang-both.pkl')\n",
        "train = load_clean_sentences('indo-minang-train.pkl')\n",
        "test = load_clean_sentences('indo-minang-test.pkl')\n",
        "# prepare indonesia tokenizer\n",
        "ind_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "ind_vocab_size = len(ind_tokenizer.word_index) + 1\n",
        "#ind_length = max_length(dataset[:, 0])\n",
        "ind_length = 9\n",
        "# prepare minang tokenizer\n",
        "min_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "min_vocab_size = len(min_tokenizer.word_index) + 1\n",
        "#min_length = max_length(dataset[:, 1])\n",
        "min_length = 9\n",
        "# prepare data\n",
        "trainX = encode_sequences(min_tokenizer, min_length, train[:, 1])\n",
        "testX = encode_sequences(min_tokenizer, min_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, ind_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, ind_tokenizer, testX, test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tugas NLP.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}